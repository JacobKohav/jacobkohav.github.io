<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>programming-language-identification on Jacob Kohav&#39;s portfolio</title>
    <link>https://jacobkohav.github.io/tags/programming-language-identification/</link>
    <description>Recent content in programming-language-identification on Jacob Kohav&#39;s portfolio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Jan 2026 03:44:58 +0000</lastBuildDate><atom:link href="https://jacobkohav.github.io/tags/programming-language-identification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On “controlling” hardware environments in throughput experiments</title>
      <link>https://jacobkohav.github.io/posts/2026-01-12-hardware-environment-control/</link>
      <pubDate>Mon, 12 Jan 2026 03:44:58 +0000</pubDate>
      
      <guid>https://jacobkohav.github.io/posts/2026-01-12-hardware-environment-control/</guid>
      <description>I was in the process of writing the next blog post on my initial experiments testing libraries for programming language detection, when I noticed a minor detail on environment setup. Inspired, I’m writing this post to share some details on how to “control” the hardware environment to create an experiment that is as replicable and consistent as possible.
Initially requesting hardware resources When beginning the experiment, I went by several criteria (which I will share in more detail in a subsequent post) to select the machine(s) which I was to use on Grid5000, an HPC commonly used by computer science researchers in France.</description>
    </item>
    
    <item>
      <title>Testing Programming Language Identification</title>
      <link>https://jacobkohav.github.io/posts/2025-12-29-testing-programming-language-identification/</link>
      <pubDate>Mon, 29 Dec 2025 03:12:58 +0000</pubDate>
      
      <guid>https://jacobkohav.github.io/posts/2025-12-29-testing-programming-language-identification/</guid>
      <description>In the previous blog post, we introduced the background, motivation, objectives, challenges, and goals of this experiment in identifying programming languages in code files.
Introduction In this post, will go through the benchmarking, testing, and analysis methods that I’m employing in more detail.
Benchmarking procedure First we’ll go through the benchmarking protocol, called Programming Language Identification (PLI) that we’ve been utilizing on the CodeCommons project [1].
As I’d gone over in the previous post, the project has been heavily making use of the Linguist dataset from GitHub for prototyping and testing prior to full execution on the Software Heritage (SWH) archive [2].</description>
    </item>
    
    <item>
      <title>Identifying Programming Languages</title>
      <link>https://jacobkohav.github.io/posts/2025-12-18-identifying-programming-languages/</link>
      <pubDate>Thu, 18 Dec 2025 22:53:58 -0700</pubDate>
      
      <guid>https://jacobkohav.github.io/posts/2025-12-18-identifying-programming-languages/</guid>
      <description>This is the beginning of a series of blog posts focusing on programming language identification in code files.
Introduction This past summer, as part of work for the
CodeCommons (CC) and Software Heritage (SWH) projects, I set out to run a series of tests benchmarking currently existing utilities that specialize in identifying programming languages (PL) in code files. In a series of blog posts, I will share my methods, results, and findings.</description>
    </item>
    
  </channel>
</rss>
