<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>library/numpy on Jacob Kohav&#39;s portfolio</title>
    <link>https://jacobkohav.github.io/tags/library/numpy/</link>
    <description>Recent content in library/numpy on Jacob Kohav&#39;s portfolio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://jacobkohav.github.io/tags/library/numpy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Testing Programming Language Identification</title>
      <link>https://jacobkohav.github.io/posts/2025-12-28-testing-programming-language-identification/</link>
      <pubDate>Mon, 29 Dec 2025 03:12:58 +0000</pubDate>
      
      <guid>https://jacobkohav.github.io/posts/2025-12-28-testing-programming-language-identification/</guid>
      <description>In the previous blog post, I introduced the background, motivation, objectives, challenges, and goals of this experiment in identifying programming languages in code files.
Introduction In this post, will go through the benchmarking, testing, and analysis methods that I’m employing in more detail.
Benchmarking procedure First we’ll go through the benchmarking protocol, called Programming Language Identification (PLI) that we’ve been utilizing on the CodeCommons project [1].
As I’d gone over in the previous post, the project has been heavily making use of the Linguist dataset from GitHub for prototyping and testing prior to full execution on the Software Heritage (SWH) archive [2].</description>
    </item>
    
  </channel>
</rss>
