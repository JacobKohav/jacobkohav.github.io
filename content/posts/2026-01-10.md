---
title: "Testing Programming Language Identification"
date: 2026-01-10T03:12:58
lastmod: 2026-01-10T03:12:58
draft: true
github_link: "https://github.com/JacobKohav"
author: "Kohav, Jacob"
tags:
  - programming-language-identification
description: ""
toc: true
---

Now that we've discussed the benchmarking and analysis methods available to use on from the Programming Language Identification (PLI) project, I'd like to share some initial tests that I've done with the tool.

## Experiement description and setup
I set up the benchmarking mechanism to record the hardware specification when running the benchmarker, as the existant PLI tooling didn't have an automated mechanism for doing so. Having such a system in place allows to systematically benchmark and compare tool programming language identification tool efficiencies.

The environment settings being recorded consistently, I set out to run the varying utilities already explored on CodeCommons on Grid5000, an HPC available to French research centers. 

### Testing infrastructure
The testing environment which I have access to is called the `Grid’5000`, an HPC available to French research centers. 

As described by themselves "Grid'5000 is a large-scale and flexible testbed for experiment-driven research in all areas of computer science, with a focus on parallel and distributed computing, including Cloud, HPC, Big Data and AI” [1].

It has “8 sites, 31 clusters, 828 nodes, 12328 cores” and 600+ regular users, along with 120 publications per year/2007 total in HAL, mainly in computer science.

It has sites in France’s large cities (and some adjacent to), including Bordeaux, Grenoble, Lille, Louvain, Luxembourg, Lyon, Nancy, Nantes, Rennes, Sophia, Strasbourg, and Toulouse. 

It’s infrastructure team is supported by members of Inria, CNRS, U. Rennes, IMT Atlantique, and U. Lorraine.

Some of it’s key features include:
- resources: “15000 cores, 800 compute-nodes grouped in homogeneous clusters, and featuring various technologies: PMEM, GPU, SSD, NVMe, 10G and 25G Ethernet, Infiniband, Omni-Path”
- reconfiguration and control allowed by a bare-metal foundation
- monitoring and measurement of network and power consumption
- infrastructure and software traceability
- a community of 600+ users
- a RESTful API to abstract infrastructure interface and automate experimentation.

### Testing environment
We'll distinguish between testing infrastructure and environment in the sense that the infrastructure is a general framework, while the environment is the specific hardware configured for the experiment.

I selected 4 NVIDIA Tesla V100-SXM2-32GB GPUs as the testing environments based on several considerations.

Experiment practices:
- a good experiment must have consistent and replicable controls, in this case, the environment.
- I would like the environment to be easily and consistently accessible to not interrupt the experiment.
- Since some utilities, especially those using machine learning, can take multiple hours to run, I would like the most powerful combination of hardware available.

The following were my constraints from Grid5000:
- use a hardware environment where I have Priority 1, otherwise my experiment/job can be terminated at any time when the resources are handed over to a colleague with higher priority
- all the GPUs that I am using must be on the same machine. While I can request GPUs on different machines, I'm actually running the code in parallel in two places, rather than at the same time from the same source. This change the results. An analogy is having two medium-power motors in a car versus one more powerful one.

The following were the resources available to me.



<!-- ![alt text](/grid5000resources.png) -->



```txt
Tue Jul 29 06:59:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:1A:00.0 Off |                    0 |
| N/A   32C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  | 00000000:1C:00.0 Off |                    0 |
| N/A   30C    P0              43W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  | 00000000:1D:00.0 Off |                    0 |
| N/A   27C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  | 00000000:1E:00.0 Off |                    0 |
| N/A   31C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

```txt
OAR_JOB_ID:               3200930
HOST_HOST_NAME:           abacus9-1.rennes.grid5000.fr
hostname:                 57c38b461397

CPU Info:
Architecture:             x86_64
  CPU op-mode(s):         32-bit, 64-bit
  Address sizes:          46 bits physical, 48 bits virtual
  Byte Order:             Little Endian
CPU(s):                   40
  On-line CPU(s) list:    0-39
Vendor ID:                GenuineIntel
  Model name:             Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz
    CPU family:           6
    Model:                85
    Thread(s) per core:   2
    Core(s) per socket:   10
    Socket(s):            2
    Stepping:             4
    CPU(s) scaling MHz:   80%
    CPU max MHz:          3000.0000
    CPU min MHz:          800.0000
    BogoMIPS:             4400.00
    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx
                           pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes6
                          4 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes x
                          save avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stib
                          p tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f 
                          avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc 
                          cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear flush_l1d arch_capab
                          ilities
Virtualization features:  
  Virtualization:         VT-x
Caches (sum of all):      
  L1d:                    640 KiB (20 instances)
  L1i:                    640 KiB (20 instances)
  L2:                     20 MiB (20 instances)
  L3:                     27.5 MiB (2 instances)
NUMA:                     
  NUMA node(s):           2
  NUMA node0 CPU(s):      0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38
  NUMA node1 CPU(s):      1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39
Vulnerabilities:          
  Gather data sampling:   Mitigation; Microcode
  Itlb multihit:          KVM: Mitigation: VMX disabled
  L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable
  Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable
  Meltdown:               Mitigation; PTI
  Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable
  Reg file data sampling: Not affected
  Retbleed:               Mitigation; IBRS
  Spec rstack overflow:   Not affected
  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp
  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
  Spectre v2:             Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected
  Srbds:                  Not affected
  Tsx async abort:        Mitigation; Clear CPU buffers; SMT vulnerable
```

### Programming language identification tools
Now let’s discuss the utilities which we’re testing:

- __GitHub Linguist__: GitHub Linguist is a rule-based and statistical language identification tool used by GitHub to detect programming languages in repositories. It combines filename extensions, shebangs, heuristics, and Bayesian classifiers trained on curated sample files. 
- __go-enry__: go-enry is a Go reimplementation and extension of GitHub Linguist that preserves its heuristic and statistical detection approach. It is optimized for performance and is commonly used in large-scale code analysis pipelines.
- __Hyperpolyglot__: Hyperpolyglot is a Rust-based reimplementation of Linguist-style language detection, emphasizing safety, speed, and easy embedding. It follows heuristic and statistical approaches.
- __cloc__: cloc (Count Lines of Code) identifies programming languages primarily to count blank, comment, and source lines across codebases. Relies on filename extensions and heuristics.
- __Pygments__: Pygments is a syntax-highlighting library that includes language detection based on filename patterns, content heuristics, and lexer guessing. 
- __Guesslang__: Guesslang is an open-source machine-learning-based programming language classifier trained on source code snippets. 
- __Magika__: Magika is a machine-learning-based file type detection tool developed by Google that infers content types directly from raw byte sequences. It can distinguish programming languages and formats without relying on filenames or extensions.







## Conclusion
Having discussed the result of the programming language identification benchmarking and the analysis format, we'll discuss initial results of programming language identification benchmarking in the next post.

## References
> [1] “PLI Benchmark,” CodeCommons GitLab repository, 20 Nov. 2024. [Online]. Available: https://gitlab.softwareheritage.org/teams/codecommons/pli-benchmark (access restricted).
>
> [2] GitHub, Inc., “Linguist sample files dataset,” GitHub Linguist repository, commit a7e40d3, 2025. [Online]. Available: https://github.com/github-linguist/linguist/tree/main/samples.

https://www.grid5000.fr/w/Grid5000:Home

https://www.grid5000.fr/mediawiki/images/Grid5000.pdf


[1] GitHub, Inc., “Linguist,” GitHub repository, 2025. [Online]. Available: https://github.com/github-linguist/linguist

[3] Google, “go-enry,” GitHub repository, 2025. [Online]. Available: https://github.com/go-enry/go-enry

[4] Hyperpolyglot contributors, “Hyperpolyglot,” GitHub repository, 2025. [Online]. Available: https://github.com/hyperpolyglot/hyperpolyglot

[5] A. Danial et al., “cloc,” GitHub repository, 2025. [Online]. Available: https://github.com/AlDanial/cloc

[6] Pygments contributors, “Pygments documentation,” Oct. 2006. [Online]. Available: https://pygments.org/

[7] Guesslang contributors, “Guesslang,” GitHub repository, 2025. [Online]. Available: https://github.com/yoeo/guesslang
[6] Y. SOMDA, “Guesslang,” PyPI project, initial releases 2017. [Online]. Available: https://pypi.org/project/guesslang/

[8] Google, “Magika: AI-powered content-type detection,” 2024. [Online]. Available: https://opensource.googleblog.com/2024/02/magika-ai-powered-content-type-detection.html

[9] Y. Fratantonio et al., “Magika: AI-Powered Content-Type Detection,” 2024. [Online]. Available: https://arxiv.org/abs/2409.13768